{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGpqF8a8oT4o7mugzVpp1W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyakapoor200/python-scrapper/blob/main/web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCV6EDkiqqHj",
        "outputId": "13be3b18-affb-46f1-d02d-4a8ac34b03db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Function to extract the Product Title\n",
        "def get_title(soup):\n",
        "    try:\n",
        "        return soup.find(\"span\", attrs={\"id\": \"productTitle\"}).text.strip()\n",
        "    except AttributeError:\n",
        "        return \"N/A\"\n",
        "\n",
        "#  Function to extract Product Price\n",
        "def get_price(soup):\n",
        "    try:\n",
        "        return soup.find(\"span\", attrs={\"class\": \"a-price-whole\"}).text.strip()\n",
        "    except AttributeError:\n",
        "        try:\n",
        "            return soup.find(\"span\", attrs={\"id\": \"priceblock_dealprice\"}).text.strip()\n",
        "        except AttributeError:\n",
        "            return \"N/A\"\n",
        "\n",
        "#  Function to extract Product Rating\n",
        "def get_rating(soup):\n",
        "    try:\n",
        "        return soup.find(\"span\", attrs={\"class\": \"a-icon-alt\"}).text.strip()\n",
        "    except AttributeError:\n",
        "        try:\n",
        "            return soup.find(\"span\", attrs={\"class\": \"a-declarative\"}).text.strip()\n",
        "        except AttributeError:\n",
        "            return \"N/A\"\n",
        "\n",
        "#  Function to extract Seller Name\n",
        "def get_seller_name(soup):\n",
        "    try:\n",
        "        return soup.find(\"a\", attrs={\"id\": \"bylineInfo\"}).text.strip()\n",
        "    except AttributeError:\n",
        "        try:\n",
        "            return soup.find(\"div\", attrs={\"id\": \"merchant-info\"}).text.strip()\n",
        "        except AttributeError:\n",
        "            return \"N/A\"\n",
        "\n",
        "#  Function to handle HTTP request retries\n",
        "def safe_request(url, headers):\n",
        "    \"\"\"Handles errors when making HTTP requests\"\"\"\n",
        "    for _ in range(3):  # Try 3 times before giving up\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            time.sleep(2)  # Wait before retrying\n",
        "        except requests.exceptions.RequestException:\n",
        "            time.sleep(2)\n",
        "    return None  # Return None if all attempts fail\n",
        "\n",
        "#  Function to scrape product links from Amazon search pages\n",
        "def get_product_links(page_num):\n",
        "    \"\"\"Extracts all product links from an Amazon search results page\"\"\"\n",
        "    HEADERS = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36',\n",
        "        'Accept-Language': 'en-US, en;q=0.5'\n",
        "    }\n",
        "\n",
        "    BASE_URL = f\"https://www.amazon.in/s?rh=n%3A6612025031&page={page_num}\"\n",
        "    response = safe_request(BASE_URL, HEADERS)\n",
        "\n",
        "    if response is None:\n",
        "        print(f\" Skipping page {page_num}, request failed!\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    product_links = [\n",
        "        \"https://www.amazon.in\" + link.get(\"href\")\n",
        "        for link in soup.find_all(\"a\", attrs={\"class\": \"a-link-normal s-no-outline\"})\n",
        "    ]\n",
        "\n",
        "    return product_links\n",
        "\n",
        "#  Function to scrape product details\n",
        "def scrape_product_data(product_url):\n",
        "    \"\"\"Extracts details of a single product\"\"\"\n",
        "    HEADERS = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36',\n",
        "        'Accept-Language': 'en-US, en;q=0.5'\n",
        "    }\n",
        "\n",
        "    product_response = safe_request(product_url, HEADERS)\n",
        "    if product_response is None:\n",
        "        return [\"N/A\", \"N/A\", \"N/A\", \"N/A\"]  # Skip product if request fails\n",
        "\n",
        "    product_soup = BeautifulSoup(product_response.content, \"html.parser\")\n",
        "\n",
        "    return [\n",
        "        get_title(product_soup),\n",
        "        get_price(product_soup),\n",
        "        get_rating(product_soup),\n",
        "        get_seller_name(product_soup)\n",
        "    ]\n",
        "\n",
        "#  Scrape all 81 pages efficiently\n",
        "final_data = {\"Product Name\": [], \"Price\": [], \"Rating\": [], \"Seller Name\": []}\n",
        "\n",
        "for page in range(1, 82):  # Scraping pages 1 to 81\n",
        "    print(f\" Scraping page {page}...\")\n",
        "\n",
        "    product_links = get_product_links(page)\n",
        "\n",
        "    for product_url in product_links:\n",
        "        product_data = scrape_product_data(product_url)\n",
        "\n",
        "        # Append scraped data\n",
        "        final_data[\"Product Name\"].append(product_data[0])\n",
        "        final_data[\"Price\"].append(product_data[1])\n",
        "        final_data[\"Rating\"].append(product_data[2])\n",
        "        final_data[\"Seller Name\"].append(product_data[3])\n",
        "\n",
        "        time.sleep(1)  # Prevent getting blocked\n",
        "\n",
        "    # Save intermediate results every 5 pages\n",
        "    if page % 5 == 0:\n",
        "        pd.DataFrame(final_data).to_csv(\"amazon_products_partial.csv\", index=False)\n",
        "        print(\" Intermediate data saved!\")\n",
        "\n",
        "#  Final Save to CSV\n",
        "df = pd.DataFrame(final_data)\n",
        "df.to_csv(\"amazon_products.csv\", index=False)\n",
        "print(\" Data scraping complete! Data saved to amazon_products.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qgJbB8P3J9L",
        "outputId": "962127f1-e0db-4103-f5f1-aac87cb28f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Scraping page 1...\n",
            " Skipping page 1, request failed!\n",
            " Scraping page 2...\n",
            " Skipping page 2, request failed!\n",
            " Scraping page 3...\n",
            " Skipping page 3, request failed!\n",
            " Scraping page 4...\n",
            " Skipping page 4, request failed!\n",
            " Scraping page 5...\n",
            " Skipping page 5, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 6...\n",
            " Scraping page 7...\n",
            " Scraping page 8...\n",
            " Skipping page 8, request failed!\n",
            " Scraping page 9...\n",
            " Scraping page 10...\n",
            " Skipping page 10, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 11...\n",
            " Skipping page 11, request failed!\n",
            " Scraping page 12...\n",
            " Skipping page 12, request failed!\n",
            " Scraping page 13...\n",
            " Skipping page 13, request failed!\n",
            " Scraping page 14...\n",
            " Skipping page 14, request failed!\n",
            " Scraping page 15...\n",
            " Skipping page 15, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 16...\n",
            " Scraping page 17...\n",
            " Skipping page 17, request failed!\n",
            " Scraping page 18...\n",
            " Skipping page 18, request failed!\n",
            " Scraping page 19...\n",
            " Skipping page 19, request failed!\n",
            " Scraping page 20...\n",
            " Intermediate data saved!\n",
            " Scraping page 21...\n",
            " Scraping page 22...\n",
            " Scraping page 23...\n",
            " Skipping page 23, request failed!\n",
            " Scraping page 24...\n",
            " Skipping page 24, request failed!\n",
            " Scraping page 25...\n",
            " Intermediate data saved!\n",
            " Scraping page 26...\n",
            " Skipping page 26, request failed!\n",
            " Scraping page 27...\n",
            " Skipping page 27, request failed!\n",
            " Scraping page 28...\n",
            " Skipping page 28, request failed!\n",
            " Scraping page 29...\n",
            " Scraping page 30...\n",
            " Skipping page 30, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 31...\n",
            " Skipping page 31, request failed!\n",
            " Scraping page 32...\n",
            " Skipping page 32, request failed!\n",
            " Scraping page 33...\n",
            " Skipping page 33, request failed!\n",
            " Scraping page 34...\n",
            " Scraping page 35...\n",
            " Skipping page 35, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 36...\n",
            " Scraping page 37...\n",
            " Skipping page 37, request failed!\n",
            " Scraping page 38...\n",
            " Skipping page 38, request failed!\n",
            " Scraping page 39...\n",
            " Skipping page 39, request failed!\n",
            " Scraping page 40...\n",
            " Skipping page 40, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 41...\n",
            " Skipping page 41, request failed!\n",
            " Scraping page 42...\n",
            " Scraping page 43...\n",
            " Scraping page 44...\n",
            " Skipping page 44, request failed!\n",
            " Scraping page 45...\n",
            " Intermediate data saved!\n",
            " Scraping page 46...\n",
            " Scraping page 47...\n",
            " Skipping page 47, request failed!\n",
            " Scraping page 48...\n",
            " Skipping page 48, request failed!\n",
            " Scraping page 49...\n",
            " Scraping page 50...\n",
            " Skipping page 50, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 51...\n",
            " Skipping page 51, request failed!\n",
            " Scraping page 52...\n",
            " Skipping page 52, request failed!\n",
            " Scraping page 53...\n",
            " Skipping page 53, request failed!\n",
            " Scraping page 54...\n",
            " Skipping page 54, request failed!\n",
            " Scraping page 55...\n",
            " Intermediate data saved!\n",
            " Scraping page 56...\n",
            " Scraping page 57...\n",
            " Skipping page 57, request failed!\n",
            " Scraping page 58...\n",
            " Scraping page 59...\n",
            " Skipping page 59, request failed!\n",
            " Scraping page 60...\n",
            " Skipping page 60, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 61...\n",
            " Scraping page 62...\n",
            " Skipping page 62, request failed!\n",
            " Scraping page 63...\n",
            " Scraping page 64...\n",
            " Skipping page 64, request failed!\n",
            " Scraping page 65...\n",
            " Skipping page 65, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 66...\n",
            " Skipping page 66, request failed!\n",
            " Scraping page 67...\n",
            " Skipping page 67, request failed!\n",
            " Scraping page 68...\n",
            " Scraping page 69...\n",
            " Skipping page 69, request failed!\n",
            " Scraping page 70...\n",
            " Intermediate data saved!\n",
            " Scraping page 71...\n",
            " Scraping page 72...\n",
            " Skipping page 72, request failed!\n",
            " Scraping page 73...\n",
            " Scraping page 74...\n",
            " Skipping page 74, request failed!\n",
            " Scraping page 75...\n",
            " Skipping page 75, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 76...\n",
            " Skipping page 76, request failed!\n",
            " Scraping page 77...\n",
            " Skipping page 77, request failed!\n",
            " Scraping page 78...\n",
            " Skipping page 78, request failed!\n",
            " Scraping page 79...\n",
            " Scraping page 80...\n",
            " Skipping page 80, request failed!\n",
            " Intermediate data saved!\n",
            " Scraping page 81...\n",
            " Skipping page 81, request failed!\n",
            " Data scraping complete! Data saved to amazon_products.csv\n"
          ]
        }
      ]
    }
  ]
}